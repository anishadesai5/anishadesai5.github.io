---
layout: post
title: Udacity AB Testing by Google 
excerpt: Final Project
permalink: /udacity_ab_testing
publish: false
tags: [HTML, CSS, Github, AB testing, Business Analytics, Hypothesis Testing]
---

<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

<h2><span style="text-decoration: underline;"><strong>Background</strong></span></h2>
<p> At the time of this experiment, Udacity courses currently have two options on the course overview page: "start free trial", and "access course materials". </p>
<li>If the student clicks “start free trial”, they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. </li>
<li>If the student clicks “access course materials”, they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.</li>

<h2><span style="text-decoration: underline;"><strong>Experiment Description</strong></span></h2>
<p> In the experiment, Udacity tested a change where if the student clicked “start free trial”, they were asked how much time they had available to devote to the course.</p>
<li>If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. </li>
<li>If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead.</li>

<img src="images/AB Testing/flowchart.png">

<p>Here is a screenshot of what the experiments look like:- </p>

<img src="images/AB Testing/screenshot.png">

<h2> Hypothesis </h2>

<li><b> Null Hypothesis: </b> This approach might not make a significant change and might not be effective in reducing the early Udacity course cancellation.</li>
<li><b> Alternative Hypothesis: </b> It might reduce the number of frustrated students who left the free trial because they didn’t have enough time, without significantly reducing the number of students to continue past the free trial and eventually complete the course.</li>

<h2> Experiment Setup </h2>
<b>Unit of Diversion: Cookie</b>

<h2> Metric Choice </h2>
<p> Two types of metrics are selected for a successful experiment: Invariant and Evaluation metrics </p>

<h3> Invariant Metrics </h3>
<p>Invariant metrics are used for sanity checks or A/A experiment before running the experiment, such as checking if the distributions are the same between control and experiment group, to make sure our experiment is not inherently wrong.
	Invariant metrics usually have a larger unit of diversion, randomly selected, or happens before the experiment starts. </p>

<li>Number of cookies: Number of unique cookies to view the course overview page. </li>
<li>Number of clicks: Number of unique cookies to click the "Start free trial" button (which happens before the free trial screener is trigger).</li>
<li>Click-through-probability: Number of unique cookies to click the "Start free trial" button divided by number of unique cookies to view the course overview page. </li>

<p>In this case, the goal of measurement is how many students will allocate more than 5 hours a week for Udacity courses, which happens before students enrolling in the courses. 
	And thus the clicks and cookies related metircs are for the invariant metircs.User-id, however, will be tracked after enrolling the course,which is not effective.</p>

<h3> Evaluation Metrics </h3>

<p> Evaluation metrics are the metrics in which we expect to see a change, and are relevant to the business goals we aim to achieve. 
	For each metric we state a Dmin - which marks the minimum change which is practically significant to the business. 
	For instance, stating that any increase in retention that is under 2%, even if statistically significant, is not practical to the business </p>

<li>Gross conversion: Number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the "Start free trial" button.</li>
<li>Retention: Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout.</li>
<li>Net conversion: Number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the "Start free trial" button.</li>
	
<p>Gross Conversion will be a good metric. 
Gross conversion means the number of enrolled divided by number of clicks. 
And thus in the experiment group, we hypothesized the number of enrollment will decrease after answering the screener questions, given those who selected <5 hour will not be encouraged to enroll.</p>

<p> Retention ratio can also be a good evaluation matric because the retention ratio in experiment group is expected to be higher because of low enrolment , 
if experiment meets the assumption. 
After seeing the message , there might be fewer users who would enrol and hence retention would be higher because it should filter out those users who can leave the course as frustrated user. </p>

<p> Net conversion is number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the ”Start free trial” button. 
	As per the intention and assumption of the experiment , experiment group users are made aware that the course require some minimum of hours each week by showing the pop message at the time of enrolment. 
	This message should filter out those users who cannot devote the required hours and are prone to be frustrated later on.
	This ratio should be different among control group users and experiment group users, if experiment’s assumptions holds true.Hence , it can be used as evaluation metric</p>

<p>Gross Conversion, will show us whether we lower our costs by introducing the new pop up. 
	Net conversion will show how the change affects our revenues. 
	After the experiment, we expect that, Gross conversion should have a practically significant decrease, and Net conversion should have a statistically significant increase.</p>
	
<h2>Measuring Standard Deviation of evaluation metrics</h2>

<p>For each of the metrics the standard deviation is calculated for a sample size of 5000 unique cookies visiting the course overview page. 
	It is asked to calculate the variability of the metrics, i.e. standard deviation of them. 
	It is probably the “standard deviation of the sampling distribution” or standard error(SE), rather than the simple SD.</p>
	
<table width="405">
<tbody>
<tr>
<td width="328">Unique cookies to view course overview page per day:</td>
<td width="77">40000</td>
</tr>
<tr>
<td>Unique cookies to click "Start free trial" per day:</td>
<td>3200</td>
</tr>
<tr>
<td>Enrollments per day:</td>
<td>660</td>
</tr>
<tr>
<td>Click-through-probability on "Start free trial":</td>
<td>0.08</td>
</tr>
<tr>
<td>Probability of enrolling, given click:</td>
<td>0.20625</td>
</tr>
<tr>
<td>Probability of payment, given enroll:</td>
<td>0.53</td>
</tr>
<tr>
<td>Probability of payment, given click</td>
<td>0.1093125</td>
</tr>
</tbody>
</table>

<p>CTR = 0.08 (given)</p>
<p>Number of users who clicked the Start Free Trial button = 5000* CTR= 5000* 0.08 = 400</p>
<p>Enrolment rate =&nbsp; Cookies to click Start Free trail/ Enrolments per day= 3200/660 = 0.20625</p>
<p>Number of users who enrolled in the free trial = 5000* CTR * Enrolment Rate = 5000*0.08*0.20625 = 82.5</p>

<img src="images/AB Testing/screenshot.png">
<p>&nbsp;</p>
<p>Gross Conversion SD = 0.0202</p>
<p>Retention Ratio SD = 0.0549</p>
<p>Net Conversion SD = 0.0156</p>

<ul>
<li>The Unit of Diversion is equal to the Unit of Analysis for Gross Conversion and Net Conversion. 
	Thus the analytical estimate would be comparable to the empirical variability.</li>
<li>For Retention the Unit of Analysis is &ldquo; 
	Number of users who enrolled in the courseware &rdquo; which is not equal to the Unit of Diversion. 
	Hence the empirical variability may be different from the Analytical estimate.</li>
</ul>

<h2>Sizing</h2>

<h3>Number of Samples vs. Power </h3>	

<p>We will not be using Bonferroni correction during analysis phase.</p>
<p>Using the <a href = "https://www.evanmiller.org/ab-testing/sample-size.html" target="_blank"> online calculator </a>		 , we calculated number of samples required as following:</p>
<p>alpha:5%&nbsp; beta:20%</p>
<p>Number of groups = 2 (control and experimental)</p>
<p>&nbsp;</p>
<p>Probability of enrolling, given click: 20.625%, base conversion rate, 1% min d.</p>
<p>Samples needed: 25,835</p>
<p>Pageviews = Samples needed * 2/CTR = 645,875</p>
<p>&nbsp;</p>
<p>Probability of payment, given enroll: 53%, base conversion rate, 1% min d.</p>
<p>Samples needed: 39,115</p>
<p>Pageviews = Samples needed * 2/CTR = 4,741,212</p>
<p>&nbsp;</p>
<p>Probability of payment, given click: 10.93125% base conversion rate, 0.75% min d.</p>
<p>Samples needed: 27,413</p>
<p>Pageviews = Samples needed * 2/CTR = 685,325</p>
<p>&nbsp;</p>
<p><em>Pageviews Required: 4,741,212</em></p>

<h3>Duration vs. Exposure</h3>

<p>If we divert 100% of traffic, given 40,000 page views per day, the experiment would take about 119 days.</p>
<p>This duration with full diversion is too time consuming for running an experiment and presents some potential risks for business, 
	such as frustrated students, lower conversion and retention, and inefficient use of coaching resources, 
	and the opportunity riks of performing other experiments.</p>
<p>Therefore, if we eliminate retention, we are left with Gross Conversion and Net Conversion, 
	and the pageview requirement will be reduced to to 685,325, and an about 18 day experiment with 100% diversion and or 35 days given 50% diversion.
	In terms of timing, an 18 day experiment is more reasonable.</p>

<h2> Experiment Analysis </h2>

<p> The experimental data can be found <a href="https://docs.google.com/spreadsheets/d/1Mu5u9GrybDdska-ljPXyBjTpdZIUev_6i7t4LRDfXM8/edit#gid=0"> here </a>
					  
<h3> Sanity Checks </h3>

<p><strong>Number of Cookies/Pageviews &amp; Number of Clicks:</strong></p>
<p><strong>&nbsp;</strong></p>
<table width="335">
<tbody>
<tr>
<td width="115">
<p><strong>&nbsp;</strong></p>
</td>
<td width="75">
<p><strong>Pageviews</strong></p>
</td>
<td width="72">
<p><strong>Clicks</strong></p>
</td>
<td width="73">
<p><strong>CTR</strong></p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>N_control</strong></p>
</td>
<td width="75">
<p>345543</p>
</td>
<td width="72">
<p>28378</p>
</td>
<td width="73">
<p>&nbsp;</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>N_experiment</strong></p>
</td>
<td width="75">
<p>344660</p>
</td>
<td width="72">
<p>28325</p>
</td>
<td width="73">
<p>&nbsp;</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>N_total</strong></p>
</td>
<td width="75">
<p>690203</p>
</td>
<td width="72">
<p>56703</p>
</td>
<td width="73">
<p>&nbsp;</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>p</strong></p>
</td>
<td width="75">
<p>0.5</p>
</td>
<td width="72">
<p>0.5</p>
</td>
<td width="73">
<p>0.08213</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>phat</strong></p>
</td>
<td width="75">
<p>0.5006</p>
</td>
<td width="72">
<p>0.5005</p>
</td>
<td width="73">
<p>0.08215</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>SE</strong></p>
</td>
<td width="75">
<p>0.0006</p>
</td>
<td width="72">
<p>0.0021</p>
</td>
<td width="73">
<p>0.00047</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>m</strong></p>
</td>
<td width="75">
<p>0.0012</p>
</td>
<td width="72">
<p>0.0041</p>
</td>
<td width="73">
<p>0.00092</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>Lower bound</strong></p>
</td>
<td width="75">
<p>0.4988</p>
</td>
<td width="72">
<p>0.4959</p>
</td>
<td width="73">
<p>0.08121</p>
</td>
</tr>
<tr>
<td width="115">
<p><strong>Upper bound</strong></p>
</td>
<td width="75">
<p>0.5012</p>
</td>
<td width="72">
<p>0.5041</p>
</td>
<td width="73">
<p>0.08304</p>
</td>
</tr>
</tbody>
</table>
<p><strong>&nbsp;</strong></p>
<p>&nbsp;</p>
<p>Since observed value of 0.5006 , 0.5005 and 0.08215 is within the expected range all metrics <strong>pass </strong>the Sanity Check.</p>
<p>&nbsp;</p>
<p>The workings are carried out in the <a href = "https://github.com/anishadesai5/anishadesai5.github.io/tree/main/workings/ABTesting" target = "_blank"> excel sheet </a> </p>

<h2> Result Analysis </h2>

<h3> Effect Size Tests </h3>

<p> A metric is statistically significant if the confidence interval does not include 0 
	(that is, you can be confident there was a change), 
	and it is practically significant if the confidence interval does not include the practical significance boundary 
	(that is, you can be confident there is a change that matters to the business.) </p>

<img src="images/AB Testing/hypothesis.png">

<p><strong>Number of Cookies/Pageviews &amp; Number of Clicks:</strong></p>
<table width="331">
<tbody>
<tr>
<td width="133">
<p><strong>&nbsp;</strong></p>
</td>
<td width="91">
<p><strong>Control</strong></p>
</td>
<td width="107">
<p><strong>Experiment</strong></p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>Enrolment</strong></p>
</td>
<td width="91">
<p>3785</p>
</td>
<td width="107">
<p>3423</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>Payment</strong></p>
</td>
<td width="91">
<p>2033</p>
</td>
<td width="107">
<p>1945</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>Total clicks till 23 customers</strong></p>
</td>
<td width="91">
<p>17293</p>
</td>
<td width="107">
<p>17260</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>dmin</strong></p>
</td>
<td width="91">
<p>0.0100</p>
</td>
<td width="107">
<p>0.0075</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>prob of gross conv</strong></p>
</td>
<td width="91">
<p>0.2189</p>
</td>
<td width="107">
<p>0.1983</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>prob of&nbsp; net conv</strong></p>
</td>
<td width="91">
<p>0.1176</p>
</td>
<td width="107">
<p>0.1127</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>phat</strong></p>
</td>
<td width="91">
<p>0.2086</p>
</td>
<td width="107">
<p>0.1151</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>SE</strong></p>
</td>
<td width="91">
<p>0.0044</p>
</td>
<td width="107">
<p>0.0034</p>
</td>
</tr>
<tr>
<td width="133">
<p><strong>m</strong></p>
</td>
<td width="91">
<p>0.0086</p>
</td>
<td width="107">
<p>0.0067</p>
</td>
</tr>
</tbody>
</table>
<p><strong>&nbsp;</strong></p>
<p>&nbsp;</p>
<table width="681">
<tbody>
<tr>
<td width="133">
<p><strong>Metric</strong></p>
</td>
<td width="91">
<p><strong>dmin</strong></p>
</td>
<td width="107">
<p><strong>Observed Difference</strong></p>
</td>
<td width="73">
<p><strong>CI Lower Bound</strong></p>
</td>
<td width="80">
<p><strong>CI Upper Bound</strong></p>
</td>
<td width="197">
<p><strong>Result</strong></p>
</td>
</tr>
<tr>
<td width="133">
<p>Gross Conversion</p>
</td>
<td width="91">
<p>0.01</p>
</td>
<td width="107">
<p>-0.0206</p>
</td>
<td width="73">
<p>-0.0291</p>
</td>
<td width="80">
<p>-0.0120</p>
</td>
<td width="197">
<p>Statistically and Practically Significant</p>
</td>
</tr>
<tr>
<td width="133">
<p>Net Conversion</p>
</td>
<td width="91">
<p>0.0075</p>
</td>
<td width="107">
<p>-0.0049</p>
</td>
<td width="73">
<p>-0.0116</p>
</td>
<td width="80">
<p>0.0019</p>
</td>
<td width="197">
<p>Neither Statistically nor Practically Significant</p>
</td>
</tr>
</tbody>
</table>

<p> Practical significance was determined based on minimum detectable effect (dmin) parameter. 
	The dmin for Gross conversion was set at 0.01 and for Net conversion was set at 0.0075. 
	For Gross conversion, our observed value is more than two below the practical significance boundary, and the confidence interval does not include zero. 
	Hence, Gross conversion is both statistically and practically significant. 
	However, for Net Conversion our observed value is well within the boundaries of practical significance, and confidence interval includes a zero. 
	Hence, Net conversion is neither statistically nor practically significant. </p>

<h3> Sign Test </h3>

<p>I used <a href="https://www.graphpad.com/quickcalcs/binomial1.cfm">online sign test calculator</a> to calculate p-values. Following are the results.</p>
<p><strong>Gross Conversion: </strong></p>
<p>alpha_individual = 0.05</p>
<table width="673">
<tbody>
<tr>
<td width="336">
<p>Number of Success- Where exp Gross Conversion is greater than control group&rsquo;s Gross conversion</p>
</td>
<td width="336">
<p>4</p>
</td>
</tr>
<tr>
<td width="336">
<p>Number of Failure- Where exp Gross Conversion is less than control group&rsquo;s Gross conversion</p>
</td>
<td width="336">
<p>19</p>
</td>
</tr>
<tr>
<td width="336">
<p>Number of days</p>
</td>
<td width="336">
<p>23</p>
</td>
</tr>
<tr>
<td width="336">
<p>Probability</p>
</td>
<td width="336">
<p>0.5</p>
</td>
</tr>
<tr>
<td width="336">
<p>Two tailed p-value</p>
</td>
<td width="336">
<p>0.0026</p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>We can see that two tailed p-value is less than our alpha value hence results are statistically significant.</p>
<p><strong>Net Conversion: </strong></p>
<p>alpha_individual = 0.025</p>
<table width="702">
<tbody>
<tr>
<td width="351">
<p>Number of Success- Where exp net conversion is greater than control group&rsquo;s Gross conversion</p>
</td>
<td width="351">
<p>10</p>
</td>
</tr>
<tr>
<td width="351">
<p>Number of Failure- Where exp net conversion is less than control group&rsquo;s Gross conversion</p>
</td>
<td width="351">
<p>13</p>
</td>
</tr>
<tr>
<td width="351">
<p>Number of days</p>
</td>
<td width="351">
<p>23</p>
</td>
</tr>
<tr>
<td width="351">
<p>Probability</p>
</td>
<td width="351">
<p>0.5</p>
</td>
</tr>
<tr>
<td width="351">
<p>Two tailed p-value</p>
</td>
<td width="351">
<p>0.6776</p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>The result is not statistically significant , which is inline with our expectations.</p>
<p><strong>&nbsp;</strong></p>
<p>&nbsp;</p>

<h3> Summary </h3>

<p>I did not use Bonferroni correction because in this case , we need both Gross Conversion and Net Retention matric to be significant (statistically and practically) 
	and if results show that any one of the matric is not significant , launch is not recommended. 
	Therefore, Bonferroni correction in this case is not necessary. Also ,in this experiment , 
	we are not considering FWER , family wise error rate, and bonferroni correction can actually lead to false negatives</p>

<h2>Recommendation </h2>
<p>My recomendation is not to launch. We can design a follow-up experiment at this point.</p>
<p>This experiment was designed to determine whether filtering students as a function of study time commitment would improve the overall 
	student experience without significantly reducing the number of students who continue past the free trial. 
	A statistically and practically signficant decrease in Gross Conversion was observed but with not in Net Conversion. 
	This translates to a decrease in enrollment not coupled to an increase in students staying for 14 days boundary to trigger payment.</p>

<p>I think udacity should not launch this change even though Udacity could improve the overall student experience and improve coaches' 
	capacity to support students who are likely to complete the course .
	This decision is in line with our hypothesis that the results might set clearer expectations for students upfront but can impact revenues(net conversion matric). 
	From the experiment results , we can see that second part of hypothesis “without significantly reducing the number of students to continue past the free trial and eventually complete the course” , 
	which pertain to Net conversion matric does not hold true. Hence , it is not recommended to take the risk and launch the change.<p>

