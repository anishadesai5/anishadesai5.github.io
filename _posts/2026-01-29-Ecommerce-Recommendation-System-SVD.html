---
layout: post
title: E-commerce Recommendation System using SVD
excerpt: Building recommendation systems with Singular Value Decomposition (SVD) for e-commerce
permalink: /ecommerce-recsys-svd
publish: false
icon: fa-shopping-cart
tags: [Machine Learning, Recommendation Systems, SVD, Matrix Factorization, E-commerce]
---

Github repository <a href = "https://github.com/anishadesai5/Ecommerice-RecSys" target="_blank">here</a>

<h2>Understanding SVD: The Intuition</h2>
<p>
In this project, I'm exploring what exactly SVD does and how matrix factorization works in recommendation systems. While I've worked with AI search indexing using semantic learning at work, I never directly implemented traditional ML-based recommendation systems in production. This is my attempt to deeply understand the fundamentals.
</p>

<h3>SVD - Singular Value Decomposition</h3>

<img src="images/svd-formula.png" alt="SVD Matrix Factorization Formula" style="max-width: 600px; display: block; margin: 20px auto;">
           
<p>
Yes, mathematical terms can be intimidating üòü But let's break it down with a simple analogy: sorting M&Ms, the candy! üç´
</p>

<h3>The Big Picture: The M&M Sorting Analogy</h3>
<img src="images/mnm-sorting.png" alt="M&M Sorting Analogy" style="max-width: 500px; display: block; margin: 20px auto;">
<p>
Imagine you're sorting M&Ms. You could analyze thousands of tiny features: dents, exact shade, position of the "m", surface texture, etc. But what's the main feature that matters? <b>Color</b> ‚Äî because it's what matters most for sorting them efficiently.
</p>
<p>
This is exactly what SVD does for ratings data:
</p>
<ul>
<li>Your data has tons of "noise" (irrelevant variations)</li>
<li>SVD finds the <b>important patterns</b> and ignores the noise</li>
<li>It's like discovering "color is what matters" automatically, without being told</li>
</ul>

<h3>The Ratings Matrix: Our Starting Point</h3>
<p>
Our user-item rating matrix looks like this:
</p>
<pre>
           iPhone  Headphones  Cable  Charger  ... (63,001 products)
Anisha        5         ?        4       ?      
Daniel        ?         2        ?       3      
John          5         ?        ?       ?      
Keren         ?         ?        3       2      
...
(192,403 users)
</pre>
<p>
<b>The problem:</b> 99.9996% of the matrix is empty! This extreme sparsity creates noise and makes finding meaningful patterns incredibly challenging.
</p>

<h3>What SVD Does: Breaking Down the Matrix</h3>
<p>
SVD factorizes our sparse ratings matrix into three smaller, denser matrices: <b>U</b>, <b>Œ£</b> (Sigma), and <b>V<sup>T</sup></b>
</p>
<p>
<code>Ratings Matrix ‚âà U √ó Œ£ √ó V<sup>T</sup></code>
</p>

<h4>1. U Matrix - "User Preference Patterns"</h4>
<p>
U contains the left singular vectors, which represent user preferences. Think of it as: <b>"What type of shopper is each user?"</b>
</p>
<pre>
           Budget  Premium  Tech-Savvy  Brand-Loyal  ... (20 patterns)
Anisha       0.1     0.9      0.8         0.3
Daniel       0.8     0.1      0.2         0.1
John         0.2     0.8      0.9         0.7
</pre>
<p>
Each number indicates how much that user aligns with each hidden pattern.
</p>

<h4>2. Œ£ Matrix - "Importance Scores"</h4>
<p>
The diagonal matrix Œ£ contains singular values ordered from high to low, telling us how important each dimension is in explaining the data. The singular values decrease with each dimension, meaning each subsequent dimension adds less value.
</p>
<pre>
Pattern 1:  25.6  ‚Üê Most important (explains 40% of behavior)
Pattern 2:  18.3  ‚Üê Second most (explains 25% of behavior)
Pattern 3:  12.1  ‚Üê Third most (explains 15%)
...
Pattern 20:  0.8  ‚Üê Barely matters (mostly noise)
</pre>

<h4>3. V Matrix - "Product Characteristic Patterns"</h4>
<p>
V contains the right singular vectors, which represent item characteristics. Think of it as: <b>"What category does each product belong to?"</b>
</p>
<pre>
              Budget  Premium  Tech-Savvy  Brand-Loyal  ... (20 patterns)
iPhone         0.1     0.9      0.9         0.8
Headphones     0.7     0.2      0.4         0.1
Cable          0.9     0.1      0.1         0.0
</pre>

<h3>How Predictions Work: The Dot Product</h3>
<p>
To predict Anisha's rating for an iPhone, we calculate the dot product of her user pattern vector with the iPhone's product pattern vector:
</p>
<pre>
Anisha's pattern:  [0.1, 0.9, 0.8, 0.3, ...]  (20 numbers)
iPhone's pattern:  [0.1, 0.9, 0.9, 0.8, ...]  (20 numbers)

Dot product = (0.1 √ó 0.1) + (0.9 √ó 0.9) + (0.8 √ó 0.9) + (0.3 √ó 0.8) + ...
            = 0.01 + 0.81 + 0.72 + 0.24 + ...
            = 3.2  ‚Üê This becomes part of the rating prediction
</pre>
<p>
<b>What's happening:</b>
</p>
<ul>
<li>Anisha is "premium" (0.9) and iPhone is "premium" (0.9) ‚Üí High match! (0.81)</li>
<li>Anisha is "tech-savvy" (0.8) and iPhone is "tech-savvy" (0.9) ‚Üí High match! (0.72)</li>
<li><b>Result:</b> Anisha will probably love the iPhone</li>
</ul>

<h3>The Three Magic Tricks SVD Does</h3>

<h4>1. Finds Important Patterns</h4>
<p>
SVD identifies the relevant sub-space within all the dimensions created by noise in the data. Like discovering that "color" matters most for M&Ms, SVD automatically discovers that "premium preference" or "tech enthusiasm" matters for electronics shoppers.
</p>

<h4>2. Ranks by Importance</h4>
<p>
Each component in the factorized matrices is ordered by importance ‚Äî u<sub>1</sub>, œÉ<sub>1</sub>, v<sub>1</sub> are the most important in describing the original matrix. This means we can keep only the top 20 patterns and ignore the rest (which are mostly noise).
</p>

<h4>3. Fills in the Blanks</h4>
<p>
Once it knows the patterns, SVD can predict missing ratings by matching user patterns with product patterns. Even though Anisha never rated AirPods, if her pattern shows she loves premium tech and AirPods have a strong premium tech pattern, SVD predicts she'll rate them highly.
</p>

<h3>Why This Solves the Sparsity Problem</h3>
<p>
Our 99.9996% sparse matrix looks impossibly empty, but it actually contains hidden patterns: premium buyers, budget hunters, tech enthusiasts, brand loyalists. SVD discovers these patterns automatically through mathematical decomposition.
</p>
<p>
<b>Example:</b>
</p>
<ul>
<li>Anisha never rated "Wireless Earbuds"</li>
<li>But SVD learned: Anisha likes premium tech (from her other ratings)</li>
<li>And SVD learned: Wireless Earbuds are premium tech (from other users' ratings)</li>
<li>Prediction: Anisha will rate Wireless Earbuds 4.8 stars!</li>
</ul>

<h3>The Bottom Line</h3>
<p>
SVD is <b>pattern matching at massive scale</b>, with patterns automatically discovered from data. It transforms an impossibly sparse problem into a manageable one by learning the hidden structure that explains why users rate items the way they do.
</p>
<p>
<b>Before SVD:</b> "Anisha rated iPhone 5 stars. Will she like AirPods?" ‚Üí "I don't know, she never rated AirPods."
</p>
<p>
<b>After SVD:</b> "Anisha's pattern shows: premium (0.9), tech (0.8), Apple brand (0.7)" + "AirPods pattern shows: premium (0.9), tech (0.9), Apple brand (0.9)" = <b>High match! Predicted rating: 4.8 stars ‚ú®</b>
</p>

<p>
Github repository <a href = "https://github.com/anishadesai5/Ecommerice-RecSys" target="_blank">here</a>
</p>
